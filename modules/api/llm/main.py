from langchain_community.document_loaders import WebBaseLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
import ollama

embeddings = OllamaEmbeddings(model='llama3')

"""Load by web"""

base_url="https://www.personeriamedellin.gov.co"
# paths = ["/nuestra-historia-anterior/mision-vision-valores/", "/nuestra-historia/", "/nuestra-historia-anterior/ideario-etico/", "/servicios/preguntas-frecuentes/"]
paths = ["/servicios/preguntas-frecuentes/"]

web_paths=[base_url + path for path in paths]

loader = WebBaseLoader(web_paths=web_paths)
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=80,
    length_function=len,
)
documents = text_splitter.split_documents(docs)

print(len(documents))

"""load by json extracted data"""

file_path = '/content/transformed.json'

# The line `vector = FAISS.from_documents(documents, embeddings)` is creating a vector representation
# of the documents using the FAISS library. This allows for efficient similarity search and retrieval
# of documents based on their embeddings generated by the OllamaEmbeddings model.
print("Creating vector representation of documents...")
vector = FAISS.from_documents(documents, embeddings)
print("Vector representation created.")

retriever = vector.as_retriever()
def combine_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

def generate_response(question, context):
  assistant_context = 'Responde solo a la pregunta anterior basado en este contenido de manera muy corta'
  formatted_prompt = f"Pregunta: {question} \n\Contexto ({assistant_context}): {context}"
  response = ollama.chat(model='llama3',
                      messages=[{'role': 'user', 'content': formatted_prompt}],
                      options = {'temperature': 0})
  return response['message']['content']

def rag_chain(question):
  retrieved_docs = retriever.invoke(question)
  print(retrieved_docs)
  formatted_context = combine_docs(retrieved_docs)
  return generate_response(question, formatted_context)

while True:
    question = input("Pregunta: ")
    if question == "exit":
        break
    result = rag_chain(question)
    print(result)